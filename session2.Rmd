---
title: "Introduction to R for text analysis"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
#bibliography: textanalysis.bib
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

# Data types in R (very briefly)

R has officially six different data types, of which four of them we will be concerned with: character, numeric, integer, and logical. We can quickly tell what kind of data type we are working with by using typeof() or mode() functions (they're slightly different but from our standpoint they will be same.)

```{r}
typeof('hello') # character
typeof(52L) # the 'L' tells R to store it as an integer
typeof(2.542) #double, ie numeric
typeof(FALSE) #logical
# if we make an assignment typeof() will tell us the data type of the underlying value
x <- 2.52; typeof(x)
y <- "Hello"; typeof(y)
```

If we try to do operations on data types that don't make any sense, R will let us know.

```{r, error = TRUE}
x + y
```

Obviously, if we ask R to tell us what 2.52 + "Hello" is, it's going to be hard to give a reasonable answer. (Notice the somewhat cryptic error message, we're going to have to get used to that, though some of the packages we'll be using are really making an effort to be more helpful.) In this case it's obvious, but as you do more and more complicated things in R with larger and larger amounts of data it will not always be the case. This is a frequent source of bugs and something to be aware of from the beginning.

Data structures are ways and forms of combining data. The simplest way of doing this is with a "vector" essentially a list (but lists are different as we'll soon see). You've already seen how to create vectors:
```{r}
c("Hello", "how", "are", "you", "?")
```

Other data structures we will work with are matrices (essentially two dimensional vectors, corpora, term frequency matrices and so on, but especially dataframes. These structures have different rules and frequently they are picky about what sort of operations they allow you to do with them, even if they look similar. Vectors and matrices, for instance can only hold one type of data. 

```{r, error = TRUE}
(c <- c(5, "hello!")) # Notice what happens here - the 5 is "coerced" into becoming a character. 
c[1] + 1 # Thus you can't do arithmetic anymore on the 5 -- 5+1=6 but "5"+1 is the same kind of operation as 2 + "Hello".
```

But you can do this with the data structure lists, which otherwise look quite similar to vectors. 

```{r}
(l <- list(5, "hello!")) # this does not coerce the 5 to become a charcter
l[[1]] + 1 # and this now works because the 5 is still a number. Notice contents of lists are subset by double [[]]
```

You can put all kinds of things together in lists and name them
```{r}
ll <- list(name = "Bob", age = 50, ingarage = head(mtcars))
names(ll)
```

Two-dimensional vectors are matrices (must have all same data type) and 2D lists are dataframes. To find what data structure we are dealing with, we type class()
```{r}
class(ll)
str(ll)
```

The str() command is also quite helpful, showing data structure, data types, names, and dimensions (which can also be found via length()). The main point to be remembered is that trying to do things with the wrong type of structure of data is a very common sources of problems. Especially as we get into more advanced text analysis packages we'll always want to keep in mind what the structures and types of data we're dealing with are.

# Beginning to work with text

There are numerous ways text can be stored in R but the most basic way, and one we will be working with today, is as a list of characters, called a "character vector" or "string". 

For an example, lets take the first several paragraphs of the lead story on nrk.no (https://www.nrk.no/norge/regjeringen-foreslar-endringer-i-statsbudsjettet-1.15721082) when this tutorial was being compiled. 

```{r}

nrk <- c("Kl 12 legger finansminister Trygve Slagsvold Vedum (Sp) fram regjeringens forslag til endringer i statsbudsjettet for neste år. Regjeringen har hatt rundt tre uker på seg til å gjøre endringer i budsjettet.", "Øker formuesskatten", "Regjeringen foreslår å øke formuesskatten fra dagens sats på 0,85 prosent til 0,95 prosent av netto formue. Samtidig økes bunnfradraget fra 1,5 til 1,65 millioner kroner, det dobbelte for ektepar.", "Formuesrabatten for aksjer og driftsmidler kuttes fra 45 prosent i 2021 til 35 prosent neste år. Det betyr at den skattemessige verdien av aksjer vil settes nærmere aksjene faktiske verdi.", "Formuesrabatten for aksjer og driftsmidler kuttes fra 45 prosent i 2021 til 35 prosent neste år. Det betyr at den skattemessige verdien av aksjer vil settes nærmere aksjene faktiske verdi.", "Regjeringen foreslår også å øke likningsverdien på fritidsboliger med 25 prosent. Dette får også betydning for formuesskatten.", "Totalt vil dette gi 3,66 milliarder kroner ekstra i statskassa. Øker drivstoffprisene mindre enn Solberg-regjeringen", "Et av de store klima-grepene til den utgående regjeringen Solberg var å øke CO₂-avgiften med 28 prosent utover forventet prisstigning.", "Det ville i så fall bety at CO₂-avgiften på bensin ville øke med 41 øre per liter.")

nrk
```

I have broken up the first several paragraphs into individual character strings and saved it as a variable called ``nrk``, thus when we "call" the variable (i.e. ask R to evaluate the variable and tell us what the result is), it prints out in four parts the text we assigned to the variable. We'll use this short, simple example to get the basics down for working with text data and producing basic word frequency counts and word clouds, then see how we do this with larger texts that we can easily download via R. 

We are going to be working with a group of R packages called the tidyverse that have been extensively developed over the last years and are exceptionally user-friendly and well-designed and documented. The tidyverse version of the dataframe is called a tibble, so we'll convert our four lines of text to a tibble. Note we have to load the library first.

```{r, message = FALSE}
library(tidyverse)
nrk <- tibble(paragraph = 1:length(nrk), text = nrk)
nrk
```

One of the first things that one will often, but not always, want to do with a text when computationally analyzing it is break it into its individual words, where each individual instance of a word is called a token. This is called "tokenizing" the text. Using the tidyverse and tidy's philosophy of data formatting (a well-developed philosophy, see https://tidyr.tidyverse.org/articles/tidy-data.html) we will tokenize the text and make each token a row in the dataframe. We do this by using the ``unnest_tokens()`` command (part of the tidytext library, thus we'll need to load this library as well) and "passing" (inputing into the function) the name we'd like to use for the column of words -- let's be boring and use the word "word" --  of the column of words it creates, and the text that it will be tokenizing and turning into columns. 

```{r}
library(tidytext)
nrk %>%
  unnest_tokens(token, text)
```

Here we used a ``%>%`` sign which is called a "pipe". Essentially what this says is to take what comes before the pipe (the variable ``nrk``) and use it as the first argument (the first input into) the following function. So it's the same as typing ``unnest_tokens(nrk, token, text)``. As we'll see shortly these pipes can be piled upon one another and save typing. 

Notice too that `unnest_tokens()`` also does some other work for us -- it turns all letters into lower case and removes punctuation. 

This is actually all we need to create a word frequency list.

```{r}
nrk %>%
  unnest_tokens(token, text) %>%
  count(token, sort = TRUE)
```

We notice though quickly that some of these words don't really mean a whole lot on their own -- til, for, fra... -- will be in any text no matter what it's about. These are called "stopwords" and often, but again not always, we'll want to get rid of them. Luckily corpus linguists have long been at work making stopword lists for many languages. 

The R package ``tm`` inludes stopword lists for multiple languages.

```{r, message = FALSE}
library(tm)
stopwords("no")
```

So what we want to do is take our ``nrk`` tibble and for every word, ie every row of the dataframe, check if it's in ``stopwords("no")`` and if it is remove it. This is accomplished with what is called an "anti-join", doing exactly what the previous sentence said. First, we turn the list (actually a character vector) of stopwords into a tibble with the column of the same name as our column of words in ``nrk`` (here called tokens) so we don't have to specify it in our anti-join.

```{r}
stop_no <- tibble(token = stopwords("no"))
nrk %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_no) %>%
  count(token, sort = TRUE)
```

This looks perhaps more informative.

The other thing we might want to do is make words such as "endring", "endringer", "endringene" into one word. This process is called "stemming" and is a somewhat more advanced process than just getting rid of stopwords. To do this we use a package in R called ``SnowballC``.

```{r}
library(SnowballC)
words_to_stem <- c("endring", "endringer", "endringene")
SnowballC::wordStem(words_to_stem, language = "no")
```

To apply this to our "corpus" of 4 paragraphs of the nrk text, we use the mutate function that creates a new column of our dataframe, the stemmed word. Then we apply our ``count()`` function to get a top frequency list.

```{r}
(nrk_stemmed <- nrk %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_no) %>%
  mutate(stemmed_token = wordStem(token, "no")) %>%
  count(stemmed_token, sort = TRUE)
)
```

Finally, with the top frequency list as a dataframe, we can then graph it using ggplot2, an extremely powerful part of the tidyverse.

```{r}
nrk_stemmed %>%
  top_n(10) %>%                     # selecting to show only top 15 words
  mutate(words = reorder(stemmed_token,n)) %>%  # this will ensure that the highest frequency words appear to the left
  ggplot(aes(words, n)) +
    geom_col() +
    coord_flip()
```

# Wordclouds

There are several packages that will take a dataframe of frequency counts and turn them into word clouds.

```{r}
library(wordcloud)
library(wordcloud2)
nrk_stemmed %>%
  with(wordcloud(stemmed_token, n, max.words = 50))
```

The `wordcloud2`` will make a little prettier, more advanced wordcloud. It wants a dataframe with one column called word and one called freq so we do this and then pass the dataframe to the function.

```{r}
data <- nrk_stemmed %>%
  rename(word = stemmed_token) %>%
  rename(freq = n) %>%
  top_n(100)
wordcloud2(data, size = 2)
```

# Larger corpora

We now have basic tools that we can apply to larger corpora. The details of reading in texts and corpora would take much longer than the time we have today but R has a number of packages that allow us to access texts that have already been digitized and cleaned. One example is the ``gutenbergr`` which allows you to access texts in the Project Gutenberg library (https://www.gutenberg.org/). If you go to the site and search books, you'll see that everyone has an ID number that we can then use to get the texts in R. We'll use the English translation of Pan, by Knut Hamsun, as our example. We'll do the same as above, with the slight difference of language for stemming and stopwording.

```{r}
library(gutenbergr)
pan <- gutenberg_download(7214, strip = TRUE)
pan
```

This is the whole novel together with a number of headers, translator's introduction, and so on. We can view the whole thing in R in a separate window with ``View(pan)``. When we do this we see that the novel itself begins on line 242 so we'll cut it down to this and then look at word frequencies.

```{r}
pan %>%
  unnest_tokens(token, text) %>%
  count(token, sort = TRUE)
```

We'll definitely want to remove stopwords. We'll use the stop_words dataframe of English stopwords in `tidytext``, thought we could also do this from the ``tm`` package in the same way as above (ie. ``tibble(token = stopwords("en"))``). We'll stem at the same time.

```{r}
pan_words <- pan %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  mutate(stemmed_token = wordStem(token, "en")) %>%
  count(stemmed_token, sort = TRUE)
pan_words
```

Now we can easily make graphs and word clouds.

```{r}
pan_words %>%
  top_n(200) %>%                     # selecting to show only top 15 words
  mutate(words = reorder(stemmed_token,n)) %>%  # this will ensure that the highest frequency words appear to the left
  ggplot(aes(words, n)) +
    geom_col() +
    coord_flip()
```

```{r}
pan_words %>%
  with(wordcloud(stemmed_token, n, max.words = 50))
```

The `wordcloud2`` will make a little prettier, more advanced wordcloud. It wants a dataframe with one column called word and one called freq so we do this and then pass the dataframe to the function.

```{r}
data <- pan_words %>%
  rename(word = stemmed_token) %>%
  rename(freq = n) %>%
  top_n(200)
wordcloud2(data, size = 2)
```